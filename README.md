# CS231n Homework

These notes accompany the Stanford CS class [CS231n: Convolutional Neural Networks for Visual Recognition](http://cs231n.stanford.edu/).

For questions/concerns/bug reports contact [Justin Johnson](http://cs.stanford.edu/people/jcjohns/) regarding the assignments, or contact [Andrej Karpathy](http://cs.stanford.edu/people/karpathy/) regarding the course notes. You can also submit a pull request directly to our [git repo](https://github.com/cs231n/cs231n.github.io). 

We encourage the use of the [hypothes.is](https://hypothes.is/) extension to annote comments and discuss these notes inline.

## Spring 2017 Assignments

[Assignment #1: Image Classification, kNN, SVM, Softmax, Neural Network](https://cs231n.github.io/assignments2017/assignment1/)

[Assignment #2: Fully-Connected Nets, Batch Normalization, Dropout, Convolutional Nets](https://cs231n.github.io/assignments2017/assignment2/)

[Assignment #3: Image Captioning with Vanilla RNNs, Image Captioning with LSTMs, Network Visualization, Style Transfer, Generative Adversarial Networks](https://cs231n.github.io/assignments2017/assignment3/)

## Module 0: Preparation

[Python / Numpy Tutorial](https://cs231n.github.io/python-numpy-tutorial/)

[IPython Notebook Tutorial](https://cs231n.github.io/ipython-tutorial/)

[Google Cloud Tutorial](https://cs231n.github.io/gce-tutorial/)

[Google Cloud with GPUs Tutorial (for assignment 2 onwards)](https://cs231n.github.io/gce-tutorial-gpus/)

[AWS Tutorial](https://cs231n.github.io/aws-tutorial/)

## Module 1: Neural Networks

[Image Classification: Data-driven Approach, k-Nearest Neighbor, train/val/test splits](https://cs231n.github.io/classification/)

L1/L2 distances, hyperparameter search, cross-validation

[Linear classification: Support Vector Machine, Softmax](https://cs231n.github.io/linear-classify/)

parameteric approach, bias trick, hinge loss, cross-entropy loss, L2 regularization, web demo

[Optimization: Stochastic Gradient Descent](https://cs231n.github.io/optimization-1/)

optimization landscapes, local search, learning rate, analytic/numerical gradient

[Backpropagation, Intuitions](https://cs231n.github.io/optimization-2/)

chain rule interpretation, real-valued circuits, patterns in gradient flow

[Neural Networks Part 1: Setting up the Architecture](https://cs231n.github.io/neural-networks-1/)

model of a biological neuron, activation functions, neural net architecture, representational power

[Neural Networks Part 2: Setting up the Data and the Loss](https://cs231n.github.io/neural-networks-2/)

preprocessing, weight initialization, batch normalization, regularization (L2/dropout), loss functions

[Neural Networks Part 3: Learning and Evaluation](https://cs231n.github.io/neural-networks-3/)

gradient checks, sanity checks, babysitting the learning process, momentum (+nesterov), second-order methods, Adagrad/RMSprop, hyperparameter optimization, model ensembles

[Putting it together: Minimal Neural Network Case Study](https://cs231n.github.io/neural-networks-case-study/)

minimal 2D toy data example

## Module 2: Convolutional Neural Networks

[Convolutional Neural Networks: Architectures, Convolution / Pooling Layers](https://cs231n.github.io/convolutional-networks/)

layers, spatial arrangement, layer patterns, layer sizing patterns, AlexNet/ZFNet/VGGNet case studies, computational considerations

[Understanding and Visualizing Convolutional Neural Networks](https://cs231n.github.io/understanding-cnn/)

tSNE embeddings, deconvnets, data gradients, fooling ConvNets, human comparisons

[Transfer Learning and Fine-tuning Convolutional Neural Networks](https://cs231n.github.io/transfer-learning/)